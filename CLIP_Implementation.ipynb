{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cchummer/ml-dl-scratch/blob/main/CLIP_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLIP implementation inspired by [these](https://keras.io/examples/vision/nl_image_search/) [two](https://towardsdatascience.com/simple-implementation-of-openai-clip-model-a-tutorial-ace6ff01d9f2) articles."
      ],
      "metadata": {
        "id": "cEkDOU6tatWH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QRnUPk_4Jytz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import itertools\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "import torch.nn.functional as F\n",
        "import albumentations as A\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iooSEJMSJxfs",
        "outputId": "738d0d82-4985-4e1f-9177-c2b8bc54e838"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8W0r0uP_tGG0"
      },
      "outputs": [],
      "source": [
        "# Preprocessing, only need to run once\n",
        "df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/ML+DL/Flickr8k/captions.txt')\n",
        "df['id'] = [id_ for id_ in range(df.shape[0] // 5) for _ in range(5)] # Deal with format of dataset (5 captions per image)\n",
        "df.to_csv('/content/drive/My Drive/Colab Notebooks/ML+DL/Flickr8k/captions.csv', index=False)\n",
        "df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/ML+DL/Flickr8k/captions.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MJLG1TCTXXg3"
      },
      "outputs": [],
      "source": [
        "# Variables needed throughout the models\n",
        "image_path = '/content/drive/My Drive/Colab Notebooks/ML+DL/Flickr8k/Images'\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "img_model_name = 'resnet50'\n",
        "text_encoder_model = \"distilbert-base-uncased\"\n",
        "text_tokenizer = \"distilbert-base-uncased\"\n",
        "tokenizer_max_len = 200\n",
        "trans_img_size = 224\n",
        "batch_size = 32\n",
        "num_workers = 2\n",
        "proj_dim_size = 256\n",
        "proj_dropout = 0.1\n",
        "image_embedding_size = 2048\n",
        "text_embedding_size = 768\n",
        "image_encoder_lr = 0.0001\n",
        "text_encoder_lr = 0.00001\n",
        "head_lr = 0.001\n",
        "weight_decay = 0.001\n",
        "scheduler_factor = 0.5\n",
        "scheduler_patience = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "MrpL35_xK-sT",
        "outputId": "36f82aae-44cc-4524-cff3-7d6baa625c8a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                           image  \\\n",
              "0      1000268201_693b08cb0e.jpg   \n",
              "1      1000268201_693b08cb0e.jpg   \n",
              "2      1000268201_693b08cb0e.jpg   \n",
              "3      1000268201_693b08cb0e.jpg   \n",
              "4      1000268201_693b08cb0e.jpg   \n",
              "...                          ...   \n",
              "40450   997722733_0cb5439472.jpg   \n",
              "40451   997722733_0cb5439472.jpg   \n",
              "40452   997722733_0cb5439472.jpg   \n",
              "40453   997722733_0cb5439472.jpg   \n",
              "40454   997722733_0cb5439472.jpg   \n",
              "\n",
              "                                                 caption    id  \n",
              "0      A child in a pink dress is climbing up a set o...     0  \n",
              "1                  A girl going into a wooden building .     0  \n",
              "2       A little girl climbing into a wooden playhouse .     0  \n",
              "3      A little girl climbing the stairs to her playh...     0  \n",
              "4      A little girl in a pink dress going into a woo...     0  \n",
              "...                                                  ...   ...  \n",
              "40450           A man in a pink shirt climbs a rock face  8090  \n",
              "40451           A man is rock climbing high in the air .  8090  \n",
              "40452  A person in a red shirt climbing up a rock fac...  8090  \n",
              "40453                    A rock climber in a red shirt .  8090  \n",
              "40454  A rock climber practices on a rock climbing wa...  8090  \n",
              "\n",
              "[40455 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cdf3cb4d-310b-4fa2-88b6-a77f39f00a1a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image</th>\n",
              "      <th>caption</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "      <td>A child in a pink dress is climbing up a set o...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "      <td>A girl going into a wooden building .</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "      <td>A little girl climbing into a wooden playhouse .</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "      <td>A little girl climbing the stairs to her playh...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "      <td>A little girl in a pink dress going into a woo...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40450</th>\n",
              "      <td>997722733_0cb5439472.jpg</td>\n",
              "      <td>A man in a pink shirt climbs a rock face</td>\n",
              "      <td>8090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40451</th>\n",
              "      <td>997722733_0cb5439472.jpg</td>\n",
              "      <td>A man is rock climbing high in the air .</td>\n",
              "      <td>8090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40452</th>\n",
              "      <td>997722733_0cb5439472.jpg</td>\n",
              "      <td>A person in a red shirt climbing up a rock fac...</td>\n",
              "      <td>8090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40453</th>\n",
              "      <td>997722733_0cb5439472.jpg</td>\n",
              "      <td>A rock climber in a red shirt .</td>\n",
              "      <td>8090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40454</th>\n",
              "      <td>997722733_0cb5439472.jpg</td>\n",
              "      <td>A rock climber practices on a rock climbing wa...</td>\n",
              "      <td>8090</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>40455 rows Ã— 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cdf3cb4d-310b-4fa2-88b6-a77f39f00a1a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cdf3cb4d-310b-4fa2-88b6-a77f39f00a1a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cdf3cb4d-310b-4fa2-88b6-a77f39f00a1a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-16b4123d-a29a-4b3c-82f6-3edba3f62910\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-16b4123d-a29a-4b3c-82f6-3edba3f62910')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-16b4123d-a29a-4b3c-82f6-3edba3f62910 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_5127a894-ae31-4062-99a6-80673f525cae\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_5127a894-ae31-4062-99a6-80673f525cae button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 40455,\n  \"fields\": [\n    {\n      \"column\": \"image\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8091,\n        \"samples\": [\n          \"3139895886_5a6d495b13.jpg\",\n          \"3133825703_359a0c414d.jpg\",\n          \"244910177_7c4ec3f65b.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"caption\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 40201,\n        \"samples\": [\n          \"A girl plays T-ball .\",\n          \"A woman in riding attire rides a jumping horse .\",\n          \"A brown dog wearing a pink shirt is followed by a brown dog wearing a yellow shirt .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2335,\n        \"min\": 0,\n        \"max\": 8090,\n        \"num_unique_values\": 8091,\n        \"samples\": [\n          4194,\n          4166,\n          1928\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/ML+DL/Flickr8k/captions.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4suTiyjWjP2"
      },
      "source": [
        "# Dataset + Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MAujvrZMWfW9"
      },
      "outputs": [],
      "source": [
        "class CLIPDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_filenames, captions, tokenizer, transforms):\n",
        "        \"\"\"\n",
        "        image_filenames and cpations must have the same length; so, if there are\n",
        "        multiple captions for each image, the image_filenames must have repetitive\n",
        "        file names\n",
        "        \"\"\"\n",
        "\n",
        "        self.image_filenames = image_filenames\n",
        "        self.captions = list(captions)\n",
        "        self.encoded_captions = tokenizer(\n",
        "            list(captions), padding=True, truncation=True, max_length=tokenizer_max_len\n",
        "        )\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {\n",
        "            key: torch.tensor(values[idx])\n",
        "            for key, values in self.encoded_captions.items()\n",
        "        }\n",
        "\n",
        "        image = cv2.imread(f\"{image_path}/{self.image_filenames[idx]}\")\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = self.transforms(image=image)['image']\n",
        "        item['image'] = torch.tensor(image).permute(2, 0, 1).float()\n",
        "        item['caption'] = self.captions[idx]\n",
        "\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.captions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hK9o4wm9ZIZ0"
      },
      "outputs": [],
      "source": [
        "def get_transforms(mode=\"train\"):\n",
        "    if mode == \"train\":\n",
        "        return A.Compose(\n",
        "            [\n",
        "                # Consider adding more augmentations for training\n",
        "                A.Resize(trans_img_size, trans_img_size, always_apply=True),\n",
        "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
        "            ]\n",
        "        )\n",
        "    else:\n",
        "        return A.Compose(\n",
        "            [\n",
        "                A.Resize(trans_img_size, trans_img_size, always_apply=True),\n",
        "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
        "            ]\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "h1SFNTtQapcZ"
      },
      "outputs": [],
      "source": [
        "def build_loaders(dataframe, tokenizer, mode):\n",
        "    transforms = get_transforms(mode=mode)\n",
        "    dataset = CLIPDataset(\n",
        "        dataframe[\"image\"].values,\n",
        "        dataframe[\"caption\"].values,\n",
        "        tokenizer=tokenizer,\n",
        "        transforms=transforms,\n",
        "    )\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        shuffle=True if mode == \"train\" else False,\n",
        "    )\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6SKLSpnPZJae"
      },
      "outputs": [],
      "source": [
        "def make_train_valid_dfs(dataframe):\n",
        "    max_id = dataframe[\"id\"].max() + 1\n",
        "    image_ids = np.arange(0, max_id)\n",
        "    np.random.seed(42)\n",
        "    valid_ids = np.random.choice(\n",
        "        image_ids, size=int(0.2 * len(image_ids)), replace=False\n",
        "    )\n",
        "    train_ids = [id_ for id_ in image_ids if id_ not in valid_ids]\n",
        "    train_dataframe = dataframe[dataframe[\"id\"].isin(train_ids)].reset_index(drop=True)\n",
        "    valid_dataframe = dataframe[dataframe[\"id\"].isin(valid_ids)].reset_index(drop=True)\n",
        "    return train_dataframe, valid_dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKk-G9_9c27z"
      },
      "source": [
        "# Models: Image + Text Encoders, Contrastive Learner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gDcx3tajc2U1"
      },
      "outputs": [],
      "source": [
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self, model_name='resnet50', pretrained=True, trainable=True\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load the pre-trained model\n",
        "        self.model = getattr(models, model_name)(pretrained=pretrained)\n",
        "\n",
        "        # Remove the final classification layer\n",
        "        self.model = nn.Sequential(*list(self.model.children())[:-1])\n",
        "\n",
        "        for p in self.model.parameters():\n",
        "            p.requires_grad = trainable\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        # Flatten the output to get a fixed size vector\n",
        "        return x.view(x.size(0), -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "M_YbIqlTdDnp"
      },
      "outputs": [],
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, model_name='distilbert-base-uncased', pretrained=True, trainable=True):\n",
        "        super().__init__()\n",
        "        if pretrained:\n",
        "            self.model = DistilBertModel.from_pretrained(model_name)\n",
        "        else:\n",
        "            self.model = DistilBertModel(config=DistilBertConfig())\n",
        "\n",
        "        for p in self.model.parameters():\n",
        "            p.requires_grad = trainable\n",
        "\n",
        "        # We are using the CLS token hidden representation as the sentence's embedding\n",
        "        self.target_token_idx = 0\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden_state = output.last_hidden_state\n",
        "        return last_hidden_state[:, self.target_token_idx, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "SxLgjR4Mgt6e"
      },
      "outputs": [],
      "source": [
        "# The projection head is used to project embeddings into tensors of the same size, so they can be compared\n",
        "# Image encoder and text encoder return tensors/embeddings of different sizes, so pass their outputs through the projection head\n",
        "class ProjectionHead(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_dim,\n",
        "        projection_dim=proj_dim_size,\n",
        "        dropout=proj_dropout\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        projected = self.projection(x)\n",
        "        x = self.gelu(projected)\n",
        "        x = self.fc(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x + projected\n",
        "        x = self.layer_norm(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "9qEHQ0Mchg7n"
      },
      "outputs": [],
      "source": [
        "def cross_entropy(preds, targets, reduction='none'):\n",
        "  log_softmax = nn.LogSoftmax(dim=-1)\n",
        "  loss = (-targets * log_softmax(preds)).sum(1)\n",
        "  if reduction == \"none\":\n",
        "      return loss\n",
        "  elif reduction == \"mean\":\n",
        "      return loss.mean()\n",
        "\n",
        "\n",
        "class CLIPModel(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      temperature=1,\n",
        "      image_embedding=image_embedding_size,\n",
        "      text_embedding=text_embedding_size,\n",
        "  ):\n",
        "    super().__init__()\n",
        "    self.image_encoder = ImageEncoder()\n",
        "    self.text_encoder = TextEncoder()\n",
        "    self.image_projection = ProjectionHead(embedding_dim=image_embedding)\n",
        "    self.text_projection = ProjectionHead(embedding_dim=text_embedding)\n",
        "    self.temperature = temperature\n",
        "\n",
        "  def forward(self, batch):\n",
        "    # Getting Image and Text Features\n",
        "    image_features = self.image_encoder(batch[\"image\"])\n",
        "    text_features = self.text_encoder(\n",
        "        input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
        "    )\n",
        "\n",
        "    # Getting Image and Text Embeddings (with same dimension)\n",
        "    image_embeddings = self.image_projection(image_features)\n",
        "    text_embeddings = self.text_projection(text_features)\n",
        "\n",
        "    # Calculating the Loss\n",
        "    logits = (text_embeddings @ image_embeddings.T) / self.temperature\n",
        "\n",
        "    images_similarity = image_embeddings @ image_embeddings.T\n",
        "    texts_similarity = text_embeddings @ text_embeddings.T\n",
        "\n",
        "    targets = F.softmax(\n",
        "        (images_similarity + texts_similarity) / 2 * self.temperature, dim=-1\n",
        "    )\n",
        "\n",
        "    texts_loss = cross_entropy(logits, targets, reduction='none')\n",
        "    images_loss = cross_entropy(logits.T, targets.T, reduction='none')\n",
        "    loss =  (images_loss + texts_loss) / 2.0 # shape: (batch_size)\n",
        "    return loss.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqT-ito9fkKC"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "RNdJvuwzg89r"
      },
      "outputs": [],
      "source": [
        "class AvgMeter:\n",
        "  def __init__(self, name=\"Metric\"):\n",
        "    self.name = name\n",
        "    self.reset()\n",
        "\n",
        "  def reset(self):\n",
        "    self.avg, self.sum, self.count = [0] * 3\n",
        "\n",
        "  def update(self, val, count=1):\n",
        "    self.count += count\n",
        "    self.sum += val * count\n",
        "    self.avg = self.sum / self.count\n",
        "\n",
        "  def __repr__(self):\n",
        "    text = f\"{self.name}: {self.avg:.4f}\"\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "k_Bfs0CjfjqJ"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, train_loader, optimizer, lr_scheduler, step):\n",
        "    loss_meter = AvgMeter()\n",
        "    i_batch = 0\n",
        "    for i, batch in enumerate(train_loader):\n",
        "\n",
        "        print(f\"Batch {i + 1} / {len(train_loader)}\")\n",
        "\n",
        "        batch = {k: v.to(device) for k, v in batch.items() if k != \"caption\"}\n",
        "\n",
        "        loss = model(batch)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if step == \"batch\":\n",
        "            lr_scheduler.step()\n",
        "\n",
        "        count = batch[\"image\"].size(0)\n",
        "        loss_meter.update(loss.item(), count)\n",
        "\n",
        "    return loss_meter\n",
        "\n",
        "def valid_epoch(model, valid_loader):\n",
        "    loss_meter = AvgMeter()\n",
        "\n",
        "    for batch in valid_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items() if k != \"caption\"}\n",
        "        loss = model(batch)\n",
        "\n",
        "        count = batch[\"image\"].size(0)\n",
        "        loss_meter.update(loss.item(), count)\n",
        "\n",
        "    return loss_meter\n",
        "\n",
        "def training_loop(df, num_epochs):\n",
        "\n",
        "    train_df, valid_df = make_train_valid_dfs(df)\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "    train_loader = build_loaders(train_df, tokenizer, mode=\"train\")\n",
        "    valid_loader = build_loaders(valid_df, tokenizer, mode=\"valid\")\n",
        "\n",
        "    model = CLIPModel().to(device)\n",
        "    params = [\n",
        "        {\"params\": model.image_encoder.parameters(), \"lr\": image_encoder_lr},\n",
        "        {\"params\": model.text_encoder.parameters(), \"lr\": text_encoder_lr},\n",
        "        {\"params\": itertools.chain(\n",
        "            model.image_projection.parameters(), model.text_projection.parameters()\n",
        "        ), \"lr\": head_lr, \"weight_decay\": weight_decay}\n",
        "    ]\n",
        "    optimizer = torch.optim.AdamW(params, weight_decay=0)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode=\"min\", patience=scheduler_patience, factor=scheduler_factor\n",
        "    )\n",
        "    step = \"epoch\"\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch: {epoch + 1}\")\n",
        "        model.train()\n",
        "\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, lr_scheduler, step)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            valid_loss = valid_epoch(model, valid_loader)\n",
        "            print(f\"Train Loss: {train_loss}\")\n",
        "            print(f\"Valid Loss: {valid_loss}\")\n",
        "\n",
        "        if valid_loss.avg < best_loss:\n",
        "            best_loss = valid_loss.avg\n",
        "            torch.save(model.state_dict(), \"best.pt\")\n",
        "            print(\"Saved Best Model!\")\n",
        "\n",
        "        lr_scheduler.step(valid_loss.avg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pfsi2G-flQs7"
      },
      "outputs": [],
      "source": [
        "training_loop(df, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "kjFXLZ1xs6Bu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# My trained model/weights\n",
        "saved_trained_path = '/content/drive/My Drive/Colab Notebooks/ML+DL/Flickr8k/best.pt'"
      ],
      "metadata": {
        "id": "9ZNm6yRfs5BV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_embeddings(valid_df, model_path):\n",
        "  tokenizer = DistilBertTokenizer.from_pretrained(text_tokenizer)\n",
        "\n",
        "  # Samples won't be shuffled when mode=valid\n",
        "  # Thus the indices of the embeddings list returned should match the order of images/filenames in the validation dataframe\n",
        "  valid_loader = build_loaders(valid_df, tokenizer, mode=\"valid\")\n",
        "\n",
        "  model = CLIPModel().to(device)\n",
        "  model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "  model.eval()\n",
        "\n",
        "  valid_image_embeddings = []\n",
        "  with torch.no_grad():\n",
        "    for batch in valid_loader:\n",
        "\n",
        "      image_features = model.image_encoder(batch[\"image\"].to(device))\n",
        "      image_embeddings = model.image_projection(image_features)\n",
        "      valid_image_embeddings.append(image_embeddings)\n",
        "\n",
        "  return model, torch.cat(valid_image_embeddings)"
      ],
      "metadata": {
        "id": "Cn6G09AkwHNd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First we grab the embeddings of all the images in our validation set\n",
        "_, valid_df = make_train_valid_dfs(df)\n",
        "model, valid_image_embeddings = get_image_embeddings(valid_df, saved_trained_path)"
      ],
      "metadata": {
        "id": "wGIIbdFswc3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_matches(model, image_embeddings, query, image_filenames, n=9):\n",
        "  tokenizer = DistilBertTokenizer.from_pretrained(text_tokenizer)\n",
        "  encoded_query = tokenizer([query])\n",
        "  batch = {\n",
        "      key: torch.tensor(values).to(device)\n",
        "      for key, values in encoded_query.items()\n",
        "  }\n",
        "  with torch.no_grad():\n",
        "    text_features = model.text_encoder(\n",
        "        input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
        "    )\n",
        "    text_embeddings = model.text_projection(text_features)\n",
        "\n",
        "  image_embeddings_n = F.normalize(image_embeddings, p=2, dim=-1)\n",
        "  text_embeddings_n = F.normalize(text_embeddings, p=2, dim=-1)\n",
        "  dot_similarity = text_embeddings_n @ image_embeddings_n.T\n",
        "\n",
        "  values, indices = torch.topk(dot_similarity.squeeze(0), n * 5)\n",
        "  matches = [image_filenames[idx] for idx in indices[::5]]\n",
        "\n",
        "  _, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
        "  for match, ax in zip(matches, axes.flatten()):\n",
        "    image = cv2.imread(f\"{image_path}/{match}\")\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    ax.imshow(image)\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "m3PPKuHrx46X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_matches(model,\n",
        "             valid_image_embeddings,\n",
        "             query=\"a dog in grass\",\n",
        "             image_filenames=valid_df['image'].values,\n",
        "             n=9)"
      ],
      "metadata": {
        "id": "h7kJvndsz1k-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPX78gG9RO8Apuc2NvpusRd",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}